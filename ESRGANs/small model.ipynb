{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\deepa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 203\u001b[0m\n\u001b[0;32m    200\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[0;32m    201\u001b[0m vgg \u001b[38;5;241m=\u001b[39m VGG19()\n\u001b[1;32m--> 203\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvgg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 145\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, vgg, dataloader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    143\u001b[0m optimizer_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Automatic mixed precision\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     fake \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     real_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator, hr, fake\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    147\u001b[0m     fake_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator, hr, fake)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_blocks(x)\n\u001b[0;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler  # Import mixed precision training tools\n",
    "\n",
    "# 1. Generator Class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_residual_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, stride=1, padding=4, bias=True)\n",
    "        self.conv1 = self.conv1.to(dtype=torch.float32)  # Explicitly set to float32\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Upsampling layers (subpixel convolution)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1, bias=True).to(dtype=torch.float32),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1, bias=True).to(dtype=torch.float32),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_channels, kernel_size=3, stride=1, padding=1, bias=True).to(dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# 2. Residual Block used in Generator\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1 = self.conv1.to(dtype=torch.float32)  # Explicitly set to float32\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2 = self.conv2.to(dtype=torch.float32)  # Explicitly set to float32\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x + residual\n",
    "\n",
    "# 3. Discriminator (PatchGAN)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4. VGG for perceptual loss\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice = nn.Sequential(*[vgg[i] for i in range(16)])  # Up to relu_4_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.slice(x)\n",
    "\n",
    "# 5. Dataset Class for HR and LR pairs\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_dir = lr_dir\n",
    "        self.transform = transform\n",
    "        self.hr_images = os.listdir(hr_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_images[idx])).convert('RGB')\n",
    "        lr_image = Image.open(os.path.join(self.lr_dir, self.hr_images[idx])).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_image = self.transform(hr_image)\n",
    "            lr_image = self.transform(lr_image)\n",
    "        \n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 6. Define Loss Functions\n",
    "def adversarial_loss(D, real, fake):\n",
    "    real_loss = torch.mean((D(real) - 1) ** 2)\n",
    "    fake_loss = torch.mean(D(fake) ** 2)\n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "def content_loss(x, y):\n",
    "    return torch.mean((x - y) ** 2)\n",
    "\n",
    "def perceptual_loss(vgg, x, y):\n",
    "    x_features = vgg(x)\n",
    "    y_features = vgg(y)\n",
    "    return content_loss(x_features, y_features)\n",
    "\n",
    "# 7. Training Loop with Image Resizing to Match HR Size\n",
    "def train(generator, discriminator, vgg, dataloader, num_epochs=50, lr=0.0002):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    vgg.to(device)\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (lr, hr) in enumerate(dataloader):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            with autocast():  # Automatic mixed precision\n",
    "                fake = generator(lr)\n",
    "                real_loss = adversarial_loss(discriminator, hr, fake.detach())\n",
    "                fake_loss = adversarial_loss(discriminator, hr, fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "            scaler.scale(d_loss).backward(retain_graph=True)  # Retain graph for the next backward pass\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            with autocast():  # Automatic mixed precision\n",
    "                fake_resized = nn.functional.interpolate(fake, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                g_loss_adv = adversarial_loss(discriminator, hr, fake_resized)\n",
    "                g_loss_content = content_loss(fake_resized, hr)\n",
    "                g_loss_perceptual = perceptual_loss(vgg, fake_resized, hr)\n",
    "                g_loss = g_loss_adv + g_loss_content + 0.006 * g_loss_perceptual\n",
    "            scaler.scale(g_loss).backward()  # No need to retain graph here as it's the last backward pass\n",
    "            scaler.step(optimizer_g)\n",
    "            scaler.update()\n",
    "\n",
    "            # Print loss every 100 steps\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n",
    "                      f\"D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "            # Free unused memory after each step\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        save_model(generator, discriminator, optimizer_g, optimizer_d, epoch, checkpoint_path=f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "# 8. Save Model Function\n",
    "def save_model(generator, discriminator, optimizer_g, optimizer_d, epoch, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "        'optimizer_d_state_dict': optimizer_d.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "# 9. Dataset and DataLoader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = SuperResolutionDataset(hr_dir=r'dataset\\small train\\high_res', \n",
    "                                       lr_dir=r'dataset\\small train\\low_res', \n",
    "                                       transform=transform)\n",
    "\n",
    "# Reduced batch size to 8 to manage memory\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# 10. Initialize models and start training\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "vgg = VGG19()\n",
    "\n",
    "train(generator, discriminator, vgg, train_dataloader, num_epochs=5, lr=0.0002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'generatort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 7. Load Generator Model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mload_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# 8. Run Testing\u001b[39;00m\n\u001b[0;32m     83\u001b[0m test(generator, test_dataloader, device\u001b[38;5;241m=\u001b[39mdevice, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mload_generator\u001b[1;34m(checkpoint_path, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator()\n\u001b[0;32m     12\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 13\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneratort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Ensure the key matches the saved state\u001b[39;00m\n\u001b[0;32m     14\u001b[0m generator\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Move to device and set to evaluation mode\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded generator model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'generatort'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. Load the Saved Generator Model\n",
    "def load_generator(checkpoint_path, device='cuda'):\n",
    "    generator = Generator()\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generatort'])  # Ensure the key matches the saved state\n",
    "    generator.to(device).eval()  # Move to device and set to evaluation mode\n",
    "    print(f\"Loaded generator model from {checkpoint_path}\")\n",
    "    return generator\n",
    "\n",
    "# 2. Dataset Class for Testing\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, lr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.transform = transform\n",
    "        self.lr_images = os.listdir(lr_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_images[idx])).convert('RGB')\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "        return lr_image, self.lr_images[idx]\n",
    "\n",
    "# 3. Test Function\n",
    "def test(generator, dataloader, device='cuda', output_dir='output_images'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    to_pil = ToPILImage()\n",
    "\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (lr, filenames) in enumerate(dataloader):\n",
    "            lr = lr.to(device)\n",
    "            fake = generator(lr)\n",
    "\n",
    "            for j in range(len(filenames)):\n",
    "                lr_image = to_pil(lr[j].cpu().detach())\n",
    "                sr_image = to_pil(fake[j].cpu().detach())\n",
    "\n",
    "                # Save images\n",
    "                lr_image.save(os.path.join(output_dir, f\"LR_{filenames[j]}\"))\n",
    "                sr_image.save(os.path.join(output_dir, f\"SR_{filenames[j]}\"))\n",
    "\n",
    "                # Display the first batch\n",
    "                if i == 0 and j == 0:\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.title(\"Low-Resolution\")\n",
    "                    plt.imshow(lr_image)\n",
    "                    plt.axis(\"off\")\n",
    "\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.title(\"Super-Resolution\")\n",
    "                    plt.imshow(sr_image)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "\n",
    "# 4. Set Paths and Load Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint_path = 'model_epoch_5.pth'  # Path to the trained generator checkpoint\n",
    "lr_test_dir = r'C:\\ESRGANs\\input'  # Path to low-resolution test images\n",
    "\n",
    "# 5. Transform for Test Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# 6. Create Test Dataset and DataLoader\n",
    "test_dataset = TestDataset(lr_dir=lr_test_dir, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# 7. Load Generator Model\n",
    "generator = load_generator(checkpoint_path, device=device)\n",
    "\n",
    "# 8. Run Testing\n",
    "test(generator, test_dataloader, device=device, output_dir='output_images')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
