{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler  # Import mixed precision training tools\n",
    "\n",
    "# 1. Generator Class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_residual_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, stride=1, padding=4)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Upsampling layers (subpixel convolution)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# 2. Residual Block used in Generator\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x + residual\n",
    "\n",
    "# 3. Discriminator (PatchGAN)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4. VGG for perceptual loss\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice = nn.Sequential(*[vgg[i] for i in range(16)])  # Up to relu_4_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.slice(x)\n",
    "\n",
    "# 5. Dataset Class for HR and LR pairs\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_dir = lr_dir\n",
    "        self.transform = transform\n",
    "        self.hr_images = os.listdir(hr_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_images[idx])).convert('RGB')\n",
    "        lr_image = Image.open(os.path.join(self.lr_dir, self.hr_images[idx])).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_image = self.transform(hr_image)\n",
    "            lr_image = self.transform(lr_image)\n",
    "        \n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 6. Define Loss Functions\n",
    "def adversarial_loss(D, real, fake):\n",
    "    real_loss = torch.mean((D(real) - 1) ** 2)\n",
    "    fake_loss = torch.mean(D(fake) ** 2)\n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "def content_loss(x, y):\n",
    "    return torch.mean((x - y) ** 2)\n",
    "\n",
    "def perceptual_loss(vgg, x, y):\n",
    "    x_features = vgg(x)\n",
    "    y_features = vgg(y)\n",
    "    return content_loss(x_features, y_features)\n",
    "\n",
    "# 7. Training Loop with Image Resizing to Match HR Size\n",
    "def train(generator, discriminator, vgg, dataloader, num_epochs=50, lr=0.0002):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    vgg.to(device)\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (lr, hr) in enumerate(dataloader):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            with autocast():\n",
    "                fake = generator(lr)\n",
    "                real_loss = adversarial_loss(discriminator, hr, fake.detach())\n",
    "                fake_loss = adversarial_loss(discriminator, hr, fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "            scaler.scale(d_loss).backward(retain_graph=True)  # Retain graph here for the next backward pass\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            with autocast():\n",
    "                # Resize fake to match hr size\n",
    "                fake_resized = nn.functional.interpolate(fake, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                g_loss_adv = adversarial_loss(discriminator, hr, fake_resized)\n",
    "                g_loss_content = content_loss(fake_resized, hr)\n",
    "                g_loss_perceptual = perceptual_loss(vgg, fake_resized, hr)\n",
    "                g_loss = g_loss_adv + g_loss_content + 0.006 * g_loss_perceptual\n",
    "            scaler.scale(g_loss).backward()  # No need to retain graph here as it's the last backward pass\n",
    "            scaler.step(optimizer_g)\n",
    "            scaler.update()\n",
    "\n",
    "            # Print loss every 100 steps\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n",
    "                      f\"D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "            # Free unused memory after each step\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. Dataset and DataLoader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = SuperResolutionDataset(hr_dir=r'dataset\\train\\high_res', \n",
    "                                       lr_dir=r'dataset\\train\\low_res', \n",
    "                                       transform=transform)\n",
    "\n",
    "# Reduced batch size to 8 to manage memory\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# 9. Initialize models and start training\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "vgg = VGG19()\n",
    "\n",
    "train(generator, discriminator, vgg, train_dataloader, num_epochs=50, lr=0.0002)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
